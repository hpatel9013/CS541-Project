{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mmamo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\mmamo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\mmamo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mmamo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Getting requirements to build wheel did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Ã— Getting requirements to build wheel did not run successfully.\n",
      "â”‚ exit code: 1\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "#%% 1. Accelerated Environment Setup\n",
    "%pip install transformers faiss-cpu sentence-transformers pandas sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"recovery-news-data.csv\")\n",
    "\n",
    "# Convert reliability to 3-tier trust score\n",
    "def get_trust_tier(row):\n",
    "    if row['news_guard_score'] >= 75 and row['mbfc_level'] == 'High':\n",
    "        return 2  # High trust\n",
    "    elif row['news_guard_score'] >= 50 or row['mbfc_level'] == 'Mixed':\n",
    "        return 1  # Medium trust\n",
    "    else:\n",
    "        return 0  # Low trust\n",
    "\n",
    "df['trust_tier'] = df.apply(get_trust_tier, axis=1)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['trust_tier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mmamo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/609 00:42 < 3:36:04, 0.05 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TrustDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TrustworthinessModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', num_labels=3)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels):\n",
    "        # Tokenize texts\n",
    "        train_encodings = self.tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "        val_encodings = self.tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = TrustDataset(train_encodings, train_labels)\n",
    "        val_dataset = TrustDataset(val_encodings, val_labels)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=16,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy='epoch',\n",
    "            logging_dir='./logs',\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))}\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "    \n",
    "    def predict(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return torch.argmax(outputs.logits, dim=1).cpu().item()\n",
    "\n",
    "# Initialize and train\n",
    "trust_model = TrustworthinessModel()\n",
    "trust_model.train(\n",
    "    train_df['body_text'].tolist(),\n",
    "    train_df['trust_tier'].tolist(),\n",
    "    val_df['body_text'].tolist(),\n",
    "    val_df['trust_tier'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class TrustAwareRetriever:\n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.index = faiss.IndexFlatL2(768)\n",
    "        self.metadata = []\n",
    "        \n",
    "    def build_index(self, documents, trust_scores):\n",
    "        # Encode documents\n",
    "        embeddings = self.encoder.encode(documents)\n",
    "        \n",
    "        # Store embeddings with trust scores\n",
    "        self.index.add(embeddings)\n",
    "        self.trust_scores = np.array(trust_scores)\n",
    "        \n",
    "    def query(self, query_text, k=10, trust_weight=0.3):\n",
    "        # Encode query\n",
    "        query_embed = self.encoder.encode([query_text])\n",
    "        \n",
    "        # Search similarity\n",
    "        distances, indices = self.index.search(query_embed, k*3)  # Over-fetch\n",
    "        \n",
    "        # Rerank with trust scores\n",
    "        results = []\n",
    "        for i, score in zip(indices[0], distances[0]):\n",
    "            trust_score = self.trust_scores[i]\n",
    "            combined_score = (1 - trust_weight) * (1 - score) + trust_weight * trust_score\n",
    "            results.append((i, combined_score))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:k]\n",
    "\n",
    "# Build index\n",
    "retriever = TrustAwareRetriever()\n",
    "retriever.build_index(\n",
    "    df['body_text'].tolist(),\n",
    "    df['trust_tier'].values / 2  # Normalize to 0-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class TrustAwareGenerator:\n",
    "    def __init__(self):\n",
    "        self.generator = pipeline('text2text-generation', model='t5-small')\n",
    "        self.trust_model = trust_model  # From previous step\n",
    "        \n",
    "    def generate(self, query, retrieved_docs, trust_threshold=1):\n",
    "        # Filter by trust\n",
    "        trusted_docs = [doc for doc in retrieved_docs \n",
    "                      if self.trust_model.predict(doc) >= trust_threshold]\n",
    "        \n",
    "        # Create context\n",
    "        context = \" | \".join(trusted_docs[:3])  # Truncate if needed\n",
    "        \n",
    "        # Generate with trust cues\n",
    "        return self.generator(\n",
    "            f\"answer: {query} context: {context}\",\n",
    "            max_length=200,\n",
    "            num_beams=4,\n",
    "            repetition_penalty=2.5\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query):\n",
    "    # Retrieve\n",
    "    retrieved = retriever.query(query)\n",
    "    doc_ids = [item[0] for item in retrieved]\n",
    "    documents = [df.iloc[i]['body_text'] for i in doc_ids]\n",
    "    \n",
    "    # Generate\n",
    "    generator = TrustAwareGenerator()\n",
    "    return generator.generate(query, documents)\n",
    "\n",
    "# Example usage\n",
    "result = rag_pipeline(\"What are the health impacts of COVID-19 vaccines?\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
