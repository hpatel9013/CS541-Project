#%% 1. Environment Setup

#%% 2. Data Preprocessing
import pandas as pd
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv("recovery-news-data.csv")

# Convert reliability to 3-tier trust score
def get_trust_tier(row):
    if row['news_guard_score'] >= 75 and row['mbfc_level'] == 'High':
        return 2  # High trust
    elif row['news_guard_score'] >= 50 or row['mbfc_level'] == 'Mixed':
        return 1  # Medium trust
    else:
        return 0  # Low trust

df['trust_tier'] = df.apply(get_trust_tier, axis=1)

# Split data
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['trust_tier'])

#%% 3. Trustworthiness Classifier
from transformers import BertTokenizer, BertForSequenceClassification
import torch

class TrustworthinessModel:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased', num_labels=3)
        
    def train(self, train_texts, train_labels, val_texts, val_labels):
        # Convert texts to BERT inputs
        train_encodings = self.tokenizer(train_texts, truncation=True, padding=True)
        val_encodings = self.tokenizer(val_texts, truncation=True, padding=True)

        # Create dataset
        class NewsDataset(torch.utils.data.Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels
            def __getitem__(self, idx):
                item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
                item['labels'] = torch.tensor(self.labels[idx])
                return item
            def __len__(self):
                return len(self.labels)
        
        train_dataset = NewsDataset(train_encodings, train_labels)
        val_dataset = NewsDataset(val_encodings, val_labels)

        # Training setup
        trainer = torch.trainer.Trainer(
            model=self.model,
            args=torch.trainer.TrainingArguments(
                per_device_train_batch_size=8,
                evaluation_strategy="epoch",
                num_train_epochs=3,
                output_dir="./trust_model"),
            train_dataset=train_dataset,
            eval_dataset=val_dataset
        )
        trainer.train()

# Initialize and train
trust_model = TrustworthinessModel()
trust_model.train(
    train_df['body_text'].tolist(),
    train_df['trust_tier'].tolist(),
    val_df['body_text'].tolist(),
    val_df['trust_tier'].tolist()
)

#%% 4. Trust-Aware Retriever
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class TrustAwareRetriever:
    def __init__(self):
        self.encoder = SentenceTransformer('all-mpnet-base-v2')
        self.index = faiss.IndexFlatL2(768)
        self.metadata = []
        
    def build_index(self, documents, trust_scores):
        # Encode documents
        embeddings = self.encoder.encode(documents)
        
        # Store embeddings with trust scores
        self.index.add(embeddings)
        self.trust_scores = np.array(trust_scores)
        
    def query(self, query_text, k=10, trust_weight=0.3):
        # Encode query
        query_embed = self.encoder.encode([query_text])
        
        # Search similarity
        distances, indices = self.index.search(query_embed, k*3)  # Over-fetch
        
        # Rerank with trust scores
        results = []
        for i, score in zip(indices[0], distances[0]):
            trust_score = self.trust_scores[i]
            combined_score = (1 - trust_weight) * (1 - score) + trust_weight * trust_score
            results.append((i, combined_score))
        
        # Sort and return top-k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:k]

# Build index
retriever = TrustAwareRetriever()
retriever.build_index(
    df['body_text'].tolist(),
    df['trust_tier'].values / 2  # Normalize to 0-1
)

#%% 5. Trust-Guided Generator
from transformers import pipeline

class TrustAwareGenerator:
    def __init__(self):
        self.generator = pipeline('text2text-generation', model='t5-small')
        self.trust_model = trust_model  # From previous step
        
    def generate(self, query, retrieved_docs, trust_threshold=1):
        # Filter by trust
        trusted_docs = [doc for doc in retrieved_docs 
                      if self.trust_model.predict(doc) >= trust_threshold]
        
        # Create context
        context = " | ".join(trusted_docs[:3])  # Truncate if needed
        
        # Generate with trust cues
        return self.generator(
            f"answer: {query} context: {context}",
            max_length=200,
            num_beams=4,
            repetition_penalty=2.5
        )

#%% 6. End-to-End Pipeline
def rag_pipeline(query):
    # Retrieve
    retrieved = retriever.query(query)
    doc_ids = [item[0] for item in retrieved]
    documents = [df.iloc[i]['body_text'] for i in doc_ids]
    
    # Generate
    generator = TrustAwareGenerator()
    return generator.generate(query, documents)

# Example usage
result = rag_pipeline("What are the health impacts of COVID-19 vaccines?")
print(result[0]['generated_text'])